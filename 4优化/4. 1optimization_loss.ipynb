{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、神经网络优化\n",
    "\n",
    "\n",
    "1. 现在的神经元模型添加了激活函数f和偏置B\n",
    "  * 激活函数：引入非线性激活因素， 提高模型的表达力。常用的激活函数：\n",
    "    * 激活函数 relu: 在 Tensorflow 中， 用 tf.nn.relu()表示\n",
    "    * 激活函数 sigmoid：在 Tensorflow 中， 用 tf.nn.sigmoid()表示\n",
    "    * 激活函数 tanh：在 Tensorflow 中， 用 tf.nn.tanh()表示\n",
    "2. 神经网络优化的参数：神经网络中所有参数 w 的个数 + 所有参数 b 的个数\n",
    "3. 神经网络的优化从损失函数loss,学习率learning_rate，滑动平均ema、正则化regu;arization这几个方面出发\n",
    "  * 损失函数loss:预测值(y)与已知值(y_)的差距,主流的损失函数有\n",
    "      * mes\n",
    "      * 自定义\n",
    "      * 交叉熵ce(Cross Entropy)\n",
    "  * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 损失函数\n",
    "* 采用官方提供的均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.80974597]\n",
      " [ 1.4852903 ]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[-0.46074435]\n",
      " [ 1.641878  ]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[-0.21939856]\n",
      " [ 1.6984766 ]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[-0.04415595]\n",
      " [ 1.7003176 ]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[0.08942621]\n",
      " [1.673328  ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[0.19583553]\n",
      " [1.6322677 ]] \n",
      "\n",
      "After 3000 training steps, w1 is: \n",
      "[[0.28375748]\n",
      " [1.5854434 ]] \n",
      "\n",
      "After 3500 training steps, w1 is: \n",
      "[[0.35848638]\n",
      " [1.5374471 ]] \n",
      "\n",
      "After 4000 training steps, w1 is: \n",
      "[[0.4233252]\n",
      " [1.4907392]] \n",
      "\n",
      "After 4500 training steps, w1 is: \n",
      "[[0.48040032]\n",
      " [1.4465573 ]] \n",
      "\n",
      "After 5000 training steps, w1 is: \n",
      "[[0.5311361]\n",
      " [1.4054534]] \n",
      "\n",
      "After 5500 training steps, w1 is: \n",
      "[[0.57653254]\n",
      " [1.367594  ]] \n",
      "\n",
      "After 6000 training steps, w1 is: \n",
      "[[0.6173259]\n",
      " [1.3329402]] \n",
      "\n",
      "After 6500 training steps, w1 is: \n",
      "[[0.65408474]\n",
      " [1.3013425 ]] \n",
      "\n",
      "After 7000 training steps, w1 is: \n",
      "[[0.68726856]\n",
      " [1.2726018 ]] \n",
      "\n",
      "After 7500 training steps, w1 is: \n",
      "[[0.7172598]\n",
      " [1.2465004]] \n",
      "\n",
      "After 8000 training steps, w1 is: \n",
      "[[0.74438614]\n",
      " [1.2228196 ]] \n",
      "\n",
      "After 8500 training steps, w1 is: \n",
      "[[0.7689325]\n",
      " [1.2013482]] \n",
      "\n",
      "After 9000 training steps, w1 is: \n",
      "[[0.79115146]\n",
      " [1.1818888 ]] \n",
      "\n",
      "After 9500 training steps, w1 is: \n",
      "[[0.81126714]\n",
      " [1.1642567 ]] \n",
      "\n",
      "After 10000 training steps, w1 is: \n",
      "[[0.8294814]\n",
      " [1.1482829]] \n",
      "\n",
      "After 10500 training steps, w1 is: \n",
      "[[0.84597576]\n",
      " [1.1338127 ]] \n",
      "\n",
      "After 11000 training steps, w1 is: \n",
      "[[0.8609128]\n",
      " [1.1207061]] \n",
      "\n",
      "After 11500 training steps, w1 is: \n",
      "[[0.87444043]\n",
      " [1.1088346 ]] \n",
      "\n",
      "After 12000 training steps, w1 is: \n",
      "[[0.88669145]\n",
      " [1.0980824 ]] \n",
      "\n",
      "After 12500 training steps, w1 is: \n",
      "[[0.8977863]\n",
      " [1.0883439]] \n",
      "\n",
      "After 13000 training steps, w1 is: \n",
      "[[0.9078348]\n",
      " [1.0795243]] \n",
      "\n",
      "After 13500 training steps, w1 is: \n",
      "[[0.91693527]\n",
      " [1.0715363 ]] \n",
      "\n",
      "After 14000 training steps, w1 is: \n",
      "[[0.92517716]\n",
      " [1.0643018 ]] \n",
      "\n",
      "After 14500 training steps, w1 is: \n",
      "[[0.93264157]\n",
      " [1.0577497 ]] \n",
      "\n",
      "After 15000 training steps, w1 is: \n",
      "[[0.9394023]\n",
      " [1.0518153]] \n",
      "\n",
      "After 15500 training steps, w1 is: \n",
      "[[0.9455251]\n",
      " [1.0464406]] \n",
      "\n",
      "After 16000 training steps, w1 is: \n",
      "[[0.95107025]\n",
      " [1.0415728 ]] \n",
      "\n",
      "After 16500 training steps, w1 is: \n",
      "[[0.9560928]\n",
      " [1.037164 ]] \n",
      "\n",
      "After 17000 training steps, w1 is: \n",
      "[[0.96064115]\n",
      " [1.0331714 ]] \n",
      "\n",
      "After 17500 training steps, w1 is: \n",
      "[[0.96476096]\n",
      " [1.0295546 ]] \n",
      "\n",
      "After 18000 training steps, w1 is: \n",
      "[[0.9684917]\n",
      " [1.0262802]] \n",
      "\n",
      "After 18500 training steps, w1 is: \n",
      "[[0.9718707]\n",
      " [1.0233142]] \n",
      "\n",
      "After 19000 training steps, w1 is: \n",
      "[[0.974931 ]\n",
      " [1.0206276]] \n",
      "\n",
      "After 19500 training steps, w1 is: \n",
      "[[0.9777026]\n",
      " [1.0181949]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.98019385]\n",
      " [1.0159807 ]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 去掉警告\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# 预测多或预测少的影响一样\n",
    "# 利润成本一样\n",
    "# 0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455   # 实际中可以不写的\n",
    "\n",
    "# 生成数据\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "# 定义损失函数为MSE,反向传播方法为梯度下降。\n",
    "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自定义损失函数 \n",
    "* 以下这个例子的目标是 ：我们定义单位的酸奶的成本相比酸奶的利润要低，所以如果预测少了的话，会使得总体利润减少，所以生成的模型应该多预测一些，我们就会多生产一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.762993 ]\n",
      " [ 1.5095658]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[1.0235443]\n",
      " [1.0463371]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[1.0174844]\n",
      " [1.0406414]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[1.0211805]\n",
      " [1.0472372]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[1.0179386]\n",
      " [1.041272 ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[1.0205938]\n",
      " [1.0390443]] \n",
      "\n",
      "Final w1 is: \n",
      " [[1.0296593]\n",
      " [1.0484141]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# 酸奶成本1元， 酸奶利润9元\n",
    "# 预测少了损失大，故不要预测少，故生成的模型会多预测一些\n",
    "# 0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1\n",
    "PROFIT = 9\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "# tf.greater询问y>y_吗？则为前面一个，否则为后面一个\n",
    "# tf.reduce_sum:将所有的损失求和\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 3生成会话，训练STEPS轮。\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print (sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从上面的代码就可以看出，两个参数均增大了，那么整体的预测值就会增大。从而达到目的。\n",
    "* 下面是单位酸奶成本高于利润的案例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.80594873]\n",
      " [ 1.4873729 ]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[0.8732146]\n",
      " [1.006204 ]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[0.9658064 ]\n",
      " [0.96982086]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[0.9645447]\n",
      " [0.9682947]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[0.9602475]\n",
      " [0.9742085]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[0.96100295]\n",
      " [0.9699342 ]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.9600407]\n",
      " [0.9733418]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# 酸奶成本9元， 酸奶利润1元\n",
    "# 预测多了损失大，故不要预测多，故生成的模型会少预测一些\n",
    "# 0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 9     # 成本\n",
    "PROFIT = 1   # 利润\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "# 重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 3生成会话，训练STEPS轮。\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.交叉熵\n",
    "* 定义:表示示两个概率分布之间的距离。\n",
    "* 特点：\n",
    "    * 交叉熵越大，两个概率分布距离越远， 两个概率分布越相异； \n",
    "    * 交叉熵越小，两个概率分布距离越近，两个概率分布越相似。\n",
    "* 计算公式：𝐇(𝐲_ , 𝐲) = −∑𝐲_ ∗ 𝒍𝒐𝒈 𝒚\n",
    "* 用 Tensorflow 函数表示为\n",
    "  ```\n",
    "  ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))\n",
    "  # 这里对输入log的值做了限制，因为概率不可能大于1\n",
    "  ```\n",
    "* 案例\n",
    ">两个神经网络模型解决二分类问题中，已知标准答案为 y_ = (1, 0)，第一个神经网络模型预测结果为y1=(0.6, 0.4)，第二个神经网络模型预测结果为 y2=(0.8, 0.2)，判断哪个神经网络模型预测的结果更接\n",
    "近标准答案。 根据交叉熵的计算公式得：\n",
    "H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) ≈ -(-0.222 + 0) = 0.222\n",
    "H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) ≈ -(-0.097 + 0) = 0.097\n",
    "由于 0.222>0.097，所以预测结果 y2 与标准答案 y_更接近， y2 预测更准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. softmax函数\n",
    "* 功能：将 n 分类的 n 个输出（ y1,y2…yn） 变为满足以下概率分布要求的函数。\n",
    ">∀𝐱 𝐏(𝐗 = 𝐱) ∈ [𝟎, 𝟏] 且 ∑ 𝑷𝒙 (𝑿 = 𝒙) = 𝟏\n",
    "* softmax 函数表示为： 𝐬𝐨𝐟𝐭𝐦𝐚𝐱(𝒚𝒊) = 𝒆^(𝒚𝒊)/∑𝒋 𝒏 =𝟏𝒆𝒚𝒊\n",
    "* softmax 函数应用： 在 n 分类中， 模型会有 n 个输出， 即 y1,y2…yn， 其中 yi 表示第 i 种情况出现的可能性大小。将 n 个输出经过 softmax 函数， 可得到符合概率分布的分类结果。\n",
    "* 在 Tensorflow 中，一般让模型的输出经过 sofemax 函数， 以获得输出分类的概率分布，再与标准\n",
    "答案对比， **求出交叉熵， 得到损失函数**，用如下函数实现：\n",
    "```\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cem = tf.reduce_mean(ce)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
