{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、ensorflow框架\n",
    "## 3.1 系列介绍了tensorfow的基本运算\n",
    "* 包括加法\n",
    "* 乘法\n",
    "* 还有定义会话后进行运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 描述加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding:utf-8\n",
    "#  数据类型\n",
    "# 去掉警告\n",
    "import os\n",
    "import numpy as np  # 科学计算模块\n",
    "import tensorflow as tf     # 引入模块\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "a = tf.constant([1.0, 2.0])  # 定义一个张量等于 [1.0,2.0]\n",
    "b = tf.constant([3.0, 4.0])  # 定义一个张量等于 [3.0,4.0]\n",
    "result = a+b                 # 实现 a加 b的加法\n",
    "print(result)                # 打印结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.描述矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_2:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0]])     # 定义一个2阶张量等于[[1.0,2.0]]\n",
    "w = tf.constant([[3.0], [4.0]])   # 定义一个2阶张量等[[3.0],[4.0]]\n",
    "y = tf.matmul(x, w)               # 实现xw矩阵乘法\n",
    "print(y)                          # 打印出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.描述定义会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_3:0\", shape=(1, 1), dtype=float32)\n",
      "[[11.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0]])  # 定义一个2阶张量等于[[1.0,2.0]]\n",
    "w = tf.constant([[3.0], [4.0]])  # 定义一个2阶张量等于[[3.0],[4.0]]\n",
    "y = tf.matmul(x, w)  # 实现xw矩阵乘法\n",
    "print(y)  # 打印出结果\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))  # 执行会话并打印出执行后的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 系列介绍了前向传播的过程。\n",
    "* 目的：**前向传播就是搭建模型的计算过程， 让模型具有推理能力， 可以针对一组输入给出相应的输出。**\n",
    "* 介绍了简单的计算过程。\n",
    "* 介绍了一个简单的推导计算过程。\n",
    "* 介绍了一些参数\n",
    "1. 参数生成的方法\n",
    "   ```python\n",
    "     tf.Variable(生成的数据的方式，标准差，均值，随机种子)\n",
    "      # 后面的变量可以有，可以没有\n",
    "   ```\n",
    "   \n",
    "   神经元线上的权重。一般随机生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "y=\n",
      " [[3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "#  前向传播，单组喂养\n",
    "# 第一组喂体积 0.7 、重量 0.5\n",
    "# 两层简单神经网络（全连接）\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义输入和参数\n",
    "x = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 定义前向传播过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 用会话计算结果\n",
    "with tf.Session() as sess:\n",
    "     # 要初始化的所有数据定义为init_op\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n",
    "    print(\"y=\\n\", sess.run(y, feed_dict={x: [[0.7, 0.5]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "y=: \n",
      " [[3.0904665]\n",
      " [1.2236414]\n",
      " [1.7270732]\n",
      " [2.2305048]]\n"
     ]
    }
   ],
   "source": [
    "# 多组喂养\n",
    "\"\"\"\n",
    "   第一组喂体积0.7、重量0.5，第二组喂体积0.2、重量0.3，\n",
    "   第三组喂体积0.3 、重量0.4，第四组喂体积0.4、重量 0.5. \n",
    "\"\"\"\n",
    "# 定义输入和参数\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 定义前向传播过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 用会话计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 变量初始化/赋初值\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n",
    "    # 喂入4组数据，输出4个答案。\n",
    "    print(\"y=: \\n\", sess.run(y, feed_dict={x: [[0.7, 0.5], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3反向传播 \n",
    "目的：**训练模型参数，在所有参数上用梯度下降，使 NN 模型在训练数据上的损失函数最小。**\n",
    "这里定义了一些重要的概念\n",
    "\n",
    "1. 损失函数loss:计算得到的预测值 y 与已知答案 y_的差距。\n",
    "   * 比较古老的一种是均方误差MSE,用 tensorflow 函数表示为：\n",
    "   ```\n",
    "   loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "   ```\n",
    "2. 在反向传播中，以减小loss值为优化目标，有梯度下降、momentum优化器、adam优化器等\n",
    "   * 这三种优化方法用 tensorflow 的函数可以表示为：\n",
    "   ```\n",
    "   rain_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "   train_step=tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)\n",
    "   train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "   ```\n",
    "       * 具体的实现原理在讲义中。\n",
    "3. 学习率：决定每次参数更新的幅度。\n",
    "   \n",
    "   优化器中都需要一个叫做学习率的参数， 使用时， 如果学习率选择过大会出现震荡不收敛的情况，如果学习率选择过小， 会出现收敛速度慢的情况。 我们可以选个比较小的值填入， 比如 0.01、 0.001。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]]\n",
      "Y_:\n",
      " [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n",
      "训练前的参数:\n",
      "w1:\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 0 training step(s), loss_mse on all data is 5.20999\n",
      "After 500 training step(s), loss_mse on all data is 0.617026\n",
      "After 1000 training step(s), loss_mse on all data is 0.392288\n",
      "After 1500 training step(s), loss_mse on all data is 0.386432\n",
      "After 2000 training step(s), loss_mse on all data is 0.384254\n",
      "After 2500 training step(s), loss_mse on all data is 0.383676\n",
      "训练后的参数：\n",
      "w1:\n",
      " [[-0.40074915  1.022511    1.0013528 ]\n",
      " [-2.1308482  -0.23977892  1.1273987 ]]\n",
      "w2:\n",
      " [[-0.44574323]\n",
      " [ 1.0492716 ]\n",
      " [-0.538676  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 8       # 一次喂入神经网络多少组数据\n",
    "SEED = 23455         # 保持结果一样\n",
    "\n",
    "# 基于seed产生随机数\n",
    "rdm = np.random.RandomState(SEED)\n",
    "# 随机数返回32行2列的矩阵，表示32组特征（体积和重量），作为输入数据集\n",
    "X = rdm.rand(32, 2)\n",
    "# 从X这个32行2列的矩阵中，取出一行，判断如果和小于1 给Y赋值1， 如果和不小于1， Y赋值0\n",
    "# 作为输入数据集的标签（正确答案）(这里是虚拟生成的数据)(体积加重量小于1)\n",
    "Y_ = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "print(\"X:\\n\", X)\n",
    "print(\"Y_:\\n\", Y_)\n",
    "\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出,定义前向传播过程\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 前向传播，矩阵相乘\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "# 均方误差\n",
    "loss_mse = tf.reduce_mean(tf.square(y - y_))\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "# train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss_mse)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    # 初始化变量，输出目前（未经训练）的参数取值。\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"训练前的参数:\")\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 训练模型。\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        # 每500次输出一次total_loss\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss_mse, feed_dict={x: X, y_: Y_})\n",
    "            print(\"After %d training step(s), loss_mse on all data is %g\" % (i, total_loss))\n",
    "    # 输出训练后的参数取值。\n",
    "    print(\"训练后的参数：\")\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
