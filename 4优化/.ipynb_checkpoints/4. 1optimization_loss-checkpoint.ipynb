{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、神经网络优化\n",
    "\n",
    "1. 现在的神经元模型添加了激活函数f和偏置B\n",
    "  * 激活函数：引入非线性激活因素， 提高模型的表达力。常用的激活函数：\n",
    "    * 激活函数 relu: 在 Tensorflow 中， 用 tf.nn.relu()表示\n",
    "    * 激活函数 sigmoid：在 Tensorflow 中， 用 tf.nn.sigmoid()表示\n",
    "    * 激活函数 tanh：在 Tensorflow 中， 用 tf.nn.tanh()表示\n",
    "2. 神经网络优化的参数：神经网络中所有参数 w 的个数 + 所有参数 b 的个数\n",
    "3. 神经网络的优化从损失函数loss,学习率learning_rate，滑动平均ema、正则化regu;arization这几个方面出发\n",
    "  * 损失函数loss:预测值(y)与已知值(y_)的差距,主流的损失函数有\n",
    "      * mes\n",
    "      * 自定义\n",
    "      * 交叉熵ce(Cross Entropy)\n",
    "  * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.80974597]\n",
      " [ 1.4852903 ]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[-0.46074435]\n",
      " [ 1.641878  ]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[-0.21939856]\n",
      " [ 1.6984766 ]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[-0.04415595]\n",
      " [ 1.7003176 ]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[0.08942621]\n",
      " [1.673328  ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[0.19583553]\n",
      " [1.6322677 ]] \n",
      "\n",
      "After 3000 training steps, w1 is: \n",
      "[[0.28375748]\n",
      " [1.5854434 ]] \n",
      "\n",
      "After 3500 training steps, w1 is: \n",
      "[[0.35848638]\n",
      " [1.5374471 ]] \n",
      "\n",
      "After 4000 training steps, w1 is: \n",
      "[[0.4233252]\n",
      " [1.4907392]] \n",
      "\n",
      "After 4500 training steps, w1 is: \n",
      "[[0.48040032]\n",
      " [1.4465573 ]] \n",
      "\n",
      "After 5000 training steps, w1 is: \n",
      "[[0.5311361]\n",
      " [1.4054534]] \n",
      "\n",
      "After 5500 training steps, w1 is: \n",
      "[[0.57653254]\n",
      " [1.367594  ]] \n",
      "\n",
      "After 6000 training steps, w1 is: \n",
      "[[0.6173259]\n",
      " [1.3329402]] \n",
      "\n",
      "After 6500 training steps, w1 is: \n",
      "[[0.65408474]\n",
      " [1.3013425 ]] \n",
      "\n",
      "After 7000 training steps, w1 is: \n",
      "[[0.68726856]\n",
      " [1.2726018 ]] \n",
      "\n",
      "After 7500 training steps, w1 is: \n",
      "[[0.7172598]\n",
      " [1.2465004]] \n",
      "\n",
      "After 8000 training steps, w1 is: \n",
      "[[0.74438614]\n",
      " [1.2228196 ]] \n",
      "\n",
      "After 8500 training steps, w1 is: \n",
      "[[0.7689325]\n",
      " [1.2013482]] \n",
      "\n",
      "After 9000 training steps, w1 is: \n",
      "[[0.79115146]\n",
      " [1.1818888 ]] \n",
      "\n",
      "After 9500 training steps, w1 is: \n",
      "[[0.81126714]\n",
      " [1.1642567 ]] \n",
      "\n",
      "After 10000 training steps, w1 is: \n",
      "[[0.8294814]\n",
      " [1.1482829]] \n",
      "\n",
      "After 10500 training steps, w1 is: \n",
      "[[0.84597576]\n",
      " [1.1338127 ]] \n",
      "\n",
      "After 11000 training steps, w1 is: \n",
      "[[0.8609128]\n",
      " [1.1207061]] \n",
      "\n",
      "After 11500 training steps, w1 is: \n",
      "[[0.87444043]\n",
      " [1.1088346 ]] \n",
      "\n",
      "After 12000 training steps, w1 is: \n",
      "[[0.88669145]\n",
      " [1.0980824 ]] \n",
      "\n",
      "After 12500 training steps, w1 is: \n",
      "[[0.8977863]\n",
      " [1.0883439]] \n",
      "\n",
      "After 13000 training steps, w1 is: \n",
      "[[0.9078348]\n",
      " [1.0795243]] \n",
      "\n",
      "After 13500 training steps, w1 is: \n",
      "[[0.91693527]\n",
      " [1.0715363 ]] \n",
      "\n",
      "After 14000 training steps, w1 is: \n",
      "[[0.92517716]\n",
      " [1.0643018 ]] \n",
      "\n",
      "After 14500 training steps, w1 is: \n",
      "[[0.93264157]\n",
      " [1.0577497 ]] \n",
      "\n",
      "After 15000 training steps, w1 is: \n",
      "[[0.9394023]\n",
      " [1.0518153]] \n",
      "\n",
      "After 15500 training steps, w1 is: \n",
      "[[0.9455251]\n",
      " [1.0464406]] \n",
      "\n",
      "After 16000 training steps, w1 is: \n",
      "[[0.95107025]\n",
      " [1.0415728 ]] \n",
      "\n",
      "After 16500 training steps, w1 is: \n",
      "[[0.9560928]\n",
      " [1.037164 ]] \n",
      "\n",
      "After 17000 training steps, w1 is: \n",
      "[[0.96064115]\n",
      " [1.0331714 ]] \n",
      "\n",
      "After 17500 training steps, w1 is: \n",
      "[[0.96476096]\n",
      " [1.0295546 ]] \n",
      "\n",
      "After 18000 training steps, w1 is: \n",
      "[[0.9684917]\n",
      " [1.0262802]] \n",
      "\n",
      "After 18500 training steps, w1 is: \n",
      "[[0.9718707]\n",
      " [1.0233142]] \n",
      "\n",
      "After 19000 training steps, w1 is: \n",
      "[[0.974931 ]\n",
      " [1.0206276]] \n",
      "\n",
      "After 19500 training steps, w1 is: \n",
      "[[0.9777026]\n",
      " [1.0181949]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.98019385]\n",
      " [1.0159807 ]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 去掉警告\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# 预测多或预测少的影响一样\n",
    "# 利润成本一样\n",
    "# 0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455   # 实际中可以不写的\n",
    "\n",
    "# 生成数据\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "# 定义损失函数为MSE,反向传播方法为梯度下降。\n",
    "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自定义损失函数 \n",
    "* 以下这个例子的目标是 ：我们定义单位的酸奶的成本相比酸奶的利润要低，所以如果预测少了的话，会使得总体利润减少，所以生成的模型应该多预测一些，我们就会多生产一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.762993 ]\n",
      " [ 1.5095658]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[1.0235443]\n",
      " [1.0463371]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[1.0174844]\n",
      " [1.0406414]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[1.0211805]\n",
      " [1.0472372]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[1.0179386]\n",
      " [1.041272 ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[1.0205938]\n",
      " [1.0390443]] \n",
      "\n",
      "Final w1 is: \n",
      " [[1.0296593]\n",
      " [1.0484141]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# 酸奶成本1元， 酸奶利润9元\n",
    "# 预测少了损失大，故不要预测少，故生成的模型会多预测一些\n",
    "# 0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1\n",
    "PROFIT = 9\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "# tf.greater询问y>y_吗？则为前面一个，否则为后面一个\n",
    "# tf.reduce_sum:将所有的损失求和\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 3生成会话，训练STEPS轮。\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print (sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
