{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å››ã€ç¥ç»ç½‘ç»œä¼˜åŒ–\n",
    "\n",
    "\n",
    "1. ç°åœ¨çš„ç¥ç»å…ƒæ¨¡å‹æ·»åŠ äº†æ¿€æ´»å‡½æ•°få’Œåç½®B\n",
    "  * æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§æ¿€æ´»å› ç´ ï¼Œ æé«˜æ¨¡å‹çš„è¡¨è¾¾åŠ›ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š\n",
    "    * æ¿€æ´»å‡½æ•° relu: åœ¨ Tensorflow ä¸­ï¼Œ ç”¨ tf.nn.relu()è¡¨ç¤º\n",
    "    * æ¿€æ´»å‡½æ•° sigmoidï¼šåœ¨ Tensorflow ä¸­ï¼Œ ç”¨ tf.nn.sigmoid()è¡¨ç¤º\n",
    "    * æ¿€æ´»å‡½æ•° tanhï¼šåœ¨ Tensorflow ä¸­ï¼Œ ç”¨ tf.nn.tanh()è¡¨ç¤º\n",
    "2. ç¥ç»ç½‘ç»œä¼˜åŒ–çš„å‚æ•°ï¼šç¥ç»ç½‘ç»œä¸­æ‰€æœ‰å‚æ•° w çš„ä¸ªæ•° + æ‰€æœ‰å‚æ•° b çš„ä¸ªæ•°\n",
    "3. ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ä»æŸå¤±å‡½æ•°loss,å­¦ä¹ ç‡learning_rateï¼Œæ»‘åŠ¨å¹³å‡emaã€æ­£åˆ™åŒ–regu;arizationè¿™å‡ ä¸ªæ–¹é¢å‡ºå‘\n",
    "  * æŸå¤±å‡½æ•°loss:é¢„æµ‹å€¼(y)ä¸å·²çŸ¥å€¼(y_)çš„å·®è·,ä¸»æµçš„æŸå¤±å‡½æ•°æœ‰\n",
    "      * mes\n",
    "      * è‡ªå®šä¹‰\n",
    "      * äº¤å‰ç†µce(Cross Entropy)\n",
    "  * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æŸå¤±å‡½æ•°\n",
    "* é‡‡ç”¨å®˜æ–¹æä¾›çš„å‡æ–¹è¯¯å·®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.80974597]\n",
      " [ 1.4852903 ]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[-0.46074435]\n",
      " [ 1.641878  ]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[-0.21939856]\n",
      " [ 1.6984766 ]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[-0.04415595]\n",
      " [ 1.7003176 ]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[0.08942621]\n",
      " [1.673328  ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[0.19583553]\n",
      " [1.6322677 ]] \n",
      "\n",
      "After 3000 training steps, w1 is: \n",
      "[[0.28375748]\n",
      " [1.5854434 ]] \n",
      "\n",
      "After 3500 training steps, w1 is: \n",
      "[[0.35848638]\n",
      " [1.5374471 ]] \n",
      "\n",
      "After 4000 training steps, w1 is: \n",
      "[[0.4233252]\n",
      " [1.4907392]] \n",
      "\n",
      "After 4500 training steps, w1 is: \n",
      "[[0.48040032]\n",
      " [1.4465573 ]] \n",
      "\n",
      "After 5000 training steps, w1 is: \n",
      "[[0.5311361]\n",
      " [1.4054534]] \n",
      "\n",
      "After 5500 training steps, w1 is: \n",
      "[[0.57653254]\n",
      " [1.367594  ]] \n",
      "\n",
      "After 6000 training steps, w1 is: \n",
      "[[0.6173259]\n",
      " [1.3329402]] \n",
      "\n",
      "After 6500 training steps, w1 is: \n",
      "[[0.65408474]\n",
      " [1.3013425 ]] \n",
      "\n",
      "After 7000 training steps, w1 is: \n",
      "[[0.68726856]\n",
      " [1.2726018 ]] \n",
      "\n",
      "After 7500 training steps, w1 is: \n",
      "[[0.7172598]\n",
      " [1.2465004]] \n",
      "\n",
      "After 8000 training steps, w1 is: \n",
      "[[0.74438614]\n",
      " [1.2228196 ]] \n",
      "\n",
      "After 8500 training steps, w1 is: \n",
      "[[0.7689325]\n",
      " [1.2013482]] \n",
      "\n",
      "After 9000 training steps, w1 is: \n",
      "[[0.79115146]\n",
      " [1.1818888 ]] \n",
      "\n",
      "After 9500 training steps, w1 is: \n",
      "[[0.81126714]\n",
      " [1.1642567 ]] \n",
      "\n",
      "After 10000 training steps, w1 is: \n",
      "[[0.8294814]\n",
      " [1.1482829]] \n",
      "\n",
      "After 10500 training steps, w1 is: \n",
      "[[0.84597576]\n",
      " [1.1338127 ]] \n",
      "\n",
      "After 11000 training steps, w1 is: \n",
      "[[0.8609128]\n",
      " [1.1207061]] \n",
      "\n",
      "After 11500 training steps, w1 is: \n",
      "[[0.87444043]\n",
      " [1.1088346 ]] \n",
      "\n",
      "After 12000 training steps, w1 is: \n",
      "[[0.88669145]\n",
      " [1.0980824 ]] \n",
      "\n",
      "After 12500 training steps, w1 is: \n",
      "[[0.8977863]\n",
      " [1.0883439]] \n",
      "\n",
      "After 13000 training steps, w1 is: \n",
      "[[0.9078348]\n",
      " [1.0795243]] \n",
      "\n",
      "After 13500 training steps, w1 is: \n",
      "[[0.91693527]\n",
      " [1.0715363 ]] \n",
      "\n",
      "After 14000 training steps, w1 is: \n",
      "[[0.92517716]\n",
      " [1.0643018 ]] \n",
      "\n",
      "After 14500 training steps, w1 is: \n",
      "[[0.93264157]\n",
      " [1.0577497 ]] \n",
      "\n",
      "After 15000 training steps, w1 is: \n",
      "[[0.9394023]\n",
      " [1.0518153]] \n",
      "\n",
      "After 15500 training steps, w1 is: \n",
      "[[0.9455251]\n",
      " [1.0464406]] \n",
      "\n",
      "After 16000 training steps, w1 is: \n",
      "[[0.95107025]\n",
      " [1.0415728 ]] \n",
      "\n",
      "After 16500 training steps, w1 is: \n",
      "[[0.9560928]\n",
      " [1.037164 ]] \n",
      "\n",
      "After 17000 training steps, w1 is: \n",
      "[[0.96064115]\n",
      " [1.0331714 ]] \n",
      "\n",
      "After 17500 training steps, w1 is: \n",
      "[[0.96476096]\n",
      " [1.0295546 ]] \n",
      "\n",
      "After 18000 training steps, w1 is: \n",
      "[[0.9684917]\n",
      " [1.0262802]] \n",
      "\n",
      "After 18500 training steps, w1 is: \n",
      "[[0.9718707]\n",
      " [1.0233142]] \n",
      "\n",
      "After 19000 training steps, w1 is: \n",
      "[[0.974931 ]\n",
      " [1.0206276]] \n",
      "\n",
      "After 19500 training steps, w1 is: \n",
      "[[0.9777026]\n",
      " [1.0181949]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.98019385]\n",
      " [1.0159807 ]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# å»æ‰è­¦å‘Š\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# é¢„æµ‹å¤šæˆ–é¢„æµ‹å°‘çš„å½±å“ä¸€æ ·\n",
    "# åˆ©æ¶¦æˆæœ¬ä¸€æ ·\n",
    "# 0å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ•°æ®é›†\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455   # å®é™…ä¸­å¯ä»¥ä¸å†™çš„\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2å®šä¹‰æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­æ–¹æ³•ã€‚\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°ä¸ºMSE,åå‘ä¼ æ’­æ–¹æ³•ä¸ºæ¢¯åº¦ä¸‹é™ã€‚\n",
    "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 3ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒSTEPSè½®\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è‡ªå®šä¹‰æŸå¤±å‡½æ•° \n",
    "* ä»¥ä¸‹è¿™ä¸ªä¾‹å­çš„ç›®æ ‡æ˜¯ ï¼šæˆ‘ä»¬å®šä¹‰å•ä½çš„é…¸å¥¶çš„æˆæœ¬ç›¸æ¯”é…¸å¥¶çš„åˆ©æ¶¦è¦ä½ï¼Œæ‰€ä»¥å¦‚æœé¢„æµ‹å°‘äº†çš„è¯ï¼Œä¼šä½¿å¾—æ€»ä½“åˆ©æ¶¦å‡å°‘ï¼Œæ‰€ä»¥ç”Ÿæˆçš„æ¨¡å‹åº”è¯¥å¤šé¢„æµ‹ä¸€äº›ï¼Œæˆ‘ä»¬å°±ä¼šå¤šç”Ÿäº§ä¸€äº›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.762993 ]\n",
      " [ 1.5095658]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[1.0235443]\n",
      " [1.0463371]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[1.0174844]\n",
      " [1.0406414]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[1.0211805]\n",
      " [1.0472372]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[1.0179386]\n",
      " [1.041272 ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[1.0205938]\n",
      " [1.0390443]] \n",
      "\n",
      "Final w1 is: \n",
      " [[1.0296593]\n",
      " [1.0484141]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# é…¸å¥¶æˆæœ¬1å…ƒï¼Œ é…¸å¥¶åˆ©æ¶¦9å…ƒ\n",
    "# é¢„æµ‹å°‘äº†æŸå¤±å¤§ï¼Œæ•…ä¸è¦é¢„æµ‹å°‘ï¼Œæ•…ç”Ÿæˆçš„æ¨¡å‹ä¼šå¤šé¢„æµ‹ä¸€äº›\n",
    "# 0å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ•°æ®é›†\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1\n",
    "PROFIT = 9\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2å®šä¹‰æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­æ–¹æ³•ã€‚\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°ä½¿å¾—é¢„æµ‹å°‘äº†çš„æŸå¤±å¤§ï¼Œäºæ˜¯æ¨¡å‹åº”è¯¥åå‘å¤šçš„æ–¹å‘é¢„æµ‹ã€‚\n",
    "# tf.greaterè¯¢é—®y>y_å—ï¼Ÿåˆ™ä¸ºå‰é¢ä¸€ä¸ªï¼Œå¦åˆ™ä¸ºåé¢ä¸€ä¸ª\n",
    "# tf.reduce_sum:å°†æ‰€æœ‰çš„æŸå¤±æ±‚å’Œ\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 3ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒSTEPSè½®ã€‚\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print (sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä»ä¸Šé¢çš„ä»£ç å°±å¯ä»¥çœ‹å‡ºï¼Œä¸¤ä¸ªå‚æ•°å‡å¢å¤§äº†ï¼Œé‚£ä¹ˆæ•´ä½“çš„é¢„æµ‹å€¼å°±ä¼šå¢å¤§ã€‚ä»è€Œè¾¾åˆ°ç›®çš„ã€‚\n",
    "* ä¸‹é¢æ˜¯å•ä½é…¸å¥¶æˆæœ¬é«˜äºåˆ©æ¶¦çš„æ¡ˆä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.80594873]\n",
      " [ 1.4873729 ]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[0.8732146]\n",
      " [1.006204 ]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[0.9658064 ]\n",
      " [0.96982086]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[0.9645447]\n",
      " [0.9682947]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[0.9602475]\n",
      " [0.9742085]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[0.96100295]\n",
      " [0.9699342 ]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.9600407]\n",
      " [0.9733418]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "# é…¸å¥¶æˆæœ¬9å…ƒï¼Œ é…¸å¥¶åˆ©æ¶¦1å…ƒ\n",
    "# é¢„æµ‹å¤šäº†æŸå¤±å¤§ï¼Œæ•…ä¸è¦é¢„æµ‹å¤šï¼Œæ•…ç”Ÿæˆçš„æ¨¡å‹ä¼šå°‘é¢„æµ‹ä¸€äº›\n",
    "# 0å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ•°æ®é›†\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 9     # æˆæœ¬\n",
    "PROFIT = 1   # åˆ©æ¶¦\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "# 1å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 2å®šä¹‰æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­æ–¹æ³•ã€‚\n",
    "# é‡æ–°å®šä¹‰æŸå¤±å‡½æ•°ï¼Œä½¿å¾—é¢„æµ‹å¤šäº†çš„æŸå¤±å¤§ï¼Œäºæ˜¯æ¨¡å‹åº”è¯¥åå‘å°‘çš„æ–¹å‘é¢„æµ‹ã€‚\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 3ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒSTEPSè½®ã€‚\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.äº¤å‰ç†µ\n",
    "* å®šä¹‰:è¡¨ç¤ºç¤ºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚\n",
    "* ç‰¹ç‚¹ï¼š\n",
    "    * äº¤å‰ç†µè¶Šå¤§ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿œï¼Œ ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸å¼‚ï¼› \n",
    "    * äº¤å‰ç†µè¶Šå°ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿‘ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸ä¼¼ã€‚\n",
    "* è®¡ç®—å…¬å¼ï¼šğ‡(ğ²_ , ğ²) = âˆ’âˆ‘ğ²_ âˆ— ğ’ğ’ğ’ˆ ğ’š\n",
    "* ç”¨ Tensorflow å‡½æ•°è¡¨ç¤ºä¸º\n",
    "  ```\n",
    "  ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))\n",
    "  # è¿™é‡Œå¯¹è¾“å…¥logçš„å€¼åšäº†é™åˆ¶ï¼Œå› ä¸ºæ¦‚ç‡ä¸å¯èƒ½å¤§äº1\n",
    "  ```\n",
    "* æ¡ˆä¾‹\n",
    ">ä¸¤ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹è§£å†³äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå·²çŸ¥æ ‡å‡†ç­”æ¡ˆä¸º y_ = (1, 0)ï¼Œç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœä¸ºy1=(0.6, 0.4)ï¼Œç¬¬äºŒä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœä¸º y2=(0.8, 0.2)ï¼Œåˆ¤æ–­å“ªä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹çš„ç»“æœæ›´æ¥\n",
    "è¿‘æ ‡å‡†ç­”æ¡ˆã€‚ æ ¹æ®äº¤å‰ç†µçš„è®¡ç®—å…¬å¼å¾—ï¼š\n",
    "H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) â‰ˆ -(-0.222 + 0) = 0.222\n",
    "H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) â‰ˆ -(-0.097 + 0) = 0.097\n",
    "ç”±äº 0.222>0.097ï¼Œæ‰€ä»¥é¢„æµ‹ç»“æœ y2 ä¸æ ‡å‡†ç­”æ¡ˆ y_æ›´æ¥è¿‘ï¼Œ y2 é¢„æµ‹æ›´å‡†ç¡®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. softmaxå‡½æ•°\n",
    "* åŠŸèƒ½ï¼šå°† n åˆ†ç±»çš„ n ä¸ªè¾“å‡ºï¼ˆ y1,y2â€¦ynï¼‰ å˜ä¸ºæ»¡è¶³ä»¥ä¸‹æ¦‚ç‡åˆ†å¸ƒè¦æ±‚çš„å‡½æ•°ã€‚\n",
    ">âˆ€ğ± ğ(ğ— = ğ±) âˆˆ [ğŸ, ğŸ] ä¸” âˆ‘ ğ‘·ğ’™ (ğ‘¿ = ğ’™) = ğŸ\n",
    "* softmax å‡½æ•°è¡¨ç¤ºä¸ºï¼š ğ¬ğ¨ğŸğ­ğ¦ğšğ±(ğ’šğ’Š) = ğ’†^(ğ’šğ’Š)/âˆ‘ğ’‹ ğ’ =ğŸğ’†ğ’šğ’Š\n",
    "* softmax å‡½æ•°åº”ç”¨ï¼š åœ¨ n åˆ†ç±»ä¸­ï¼Œ æ¨¡å‹ä¼šæœ‰ n ä¸ªè¾“å‡ºï¼Œ å³ y1,y2â€¦ynï¼Œ å…¶ä¸­ yi è¡¨ç¤ºç¬¬ i ç§æƒ…å†µå‡ºç°çš„å¯èƒ½æ€§å¤§å°ã€‚å°† n ä¸ªè¾“å‡ºç»è¿‡ softmax å‡½æ•°ï¼Œ å¯å¾—åˆ°ç¬¦åˆæ¦‚ç‡åˆ†å¸ƒçš„åˆ†ç±»ç»“æœã€‚\n",
    "* åœ¨ Tensorflow ä¸­ï¼Œä¸€èˆ¬è®©æ¨¡å‹çš„è¾“å‡ºç»è¿‡ sofemax å‡½æ•°ï¼Œ ä»¥è·å¾—è¾“å‡ºåˆ†ç±»çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå†ä¸æ ‡å‡†\n",
    "ç­”æ¡ˆå¯¹æ¯”ï¼Œ **æ±‚å‡ºäº¤å‰ç†µï¼Œ å¾—åˆ°æŸå¤±å‡½æ•°**ï¼Œç”¨å¦‚ä¸‹å‡½æ•°å®ç°ï¼š\n",
    "```\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cem = tf.reduce_mean(ce)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
